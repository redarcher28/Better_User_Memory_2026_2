import os
import queue
import threading
import time
from openai import OpenAI
from dotenv import load_dotenv
from typing import List, Dict

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
try:
    load_dotenv()
except FileNotFoundError:
    print("è­¦å‘Šï¼šæœªæ‰¾åˆ° .env æ–‡ä»¶ï¼Œå°†ä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡ã€‚")
except Exception as e:
    print(f"è­¦å‘Šï¼šåŠ è½½ .env æ–‡ä»¶æ—¶å‡ºé”™: {e}")

class LLMCompatibleClient:
    """
    ç”¨ä»»ä½•å…¼å®¹OpenAIæ¥å£çš„æœåŠ¡ï¼Œå¹¶é»˜è®¤ä½¿ç”¨æµå¼å“åº”ã€‚
    """
    def __init__(self, model: str = None, apiKey: str = None, baseUrl: str = None, timeout: int = None):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯ã€‚ä¼˜å…ˆä½¿ç”¨ä¼ å…¥å‚æ•°ï¼Œå¦‚æœæœªæä¾›ï¼Œåˆ™ä»ç¯å¢ƒå˜é‡åŠ è½½ã€‚
        """
        self.model = model or os.getenv("LLM_MODEL_ID")
        apiKey = apiKey or os.getenv("LLM_API_KEY")
        baseUrl = baseUrl or os.getenv("LLM_BASE_URL")
        timeout = timeout or int(os.getenv("LLM_TIMEOUT", 60))
        self.stream_read_timeout = int(os.getenv("LLM_STREAM_READ_TIMEOUT", 120))

        if not all([self.model, apiKey, baseUrl]):
            raise ValueError("æ¨¡å‹IDã€APIå¯†é’¥å’ŒæœåŠ¡åœ°å€å¿…é¡»è¢«æä¾›æˆ–åœ¨.envæ–‡ä»¶ä¸­å®šä¹‰ã€‚")

        self.client = OpenAI(api_key=apiKey, base_url=baseUrl, timeout=timeout)

    # route: 1-1-2 è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ€è€ƒï¼Œå¹¶è¿”å›å…¶å“åº”ã€‚ messages: æç¤ºè¯é›†  temperatureï¼š æ¸©åº¦
    def think(self, messages: List[Dict[str, str]], temperature: float = 0) -> str:
        """
        è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ€è€ƒï¼Œå¹¶è¿”å›å…¶å“åº”ã€‚
        para:
        messages: List[Dict[str, str]]æç¤ºè¯é›†, æ ¼å¼å¦‚ä¸‹ï¼š

        """
        print(f"ğŸ§  æ­£åœ¨è°ƒç”¨ {self.model} æ¨¡å‹...")
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                stream=True,
            )
            # æµå¼è¯»å–å¸¦è¶…æ—¶ï¼šå­çº¿ç¨‹å¾€é˜Ÿåˆ—æ”¾ chunkï¼Œä¸»çº¿ç¨‹å¸¦è¶…æ—¶å–ï¼Œé¿å…â€œå“åº”æˆåŠŸâ€åé•¿æ—¶é—´æ— å†…å®¹
            chunk_queue = queue.Queue()
            stream_error = []

            def consume_stream():
                try:
                    for chunk in response:
                        content = chunk.choices[0].delta.content or ""
                        if content:
                            chunk_queue.put(content)
                    chunk_queue.put(None)
                except Exception as e:
                    stream_error.append(e)
                    chunk_queue.put(("__error__", e))

            reader = threading.Thread(target=consume_stream, daemon=True)
            reader.start()
            collected_content = []
            start_time = time.monotonic()
            first_chunk = True
            while True:
                remaining = self.stream_read_timeout - (time.monotonic() - start_time)
                if remaining <= 0:
                    print("\nâŒ æµå¼è¯»å–è¶…æ—¶ï¼šè¶…è¿‡ {} ç§’æœªå®Œæˆã€‚".format(self.stream_read_timeout))
                    return None
                try:
                    item = chunk_queue.get(timeout=min(60, remaining))
                except queue.Empty:
                    print("\nâŒ æµå¼è¯»å–è¶…æ—¶ï¼šç­‰å¾…ä¸‹ä¸€å—å†…å®¹è¶…æ—¶ã€‚")
                    return None
                if item is None:
                    break
                if isinstance(item, tuple) and item[0] == "__error__":
                    print(f"\nâŒ è°ƒç”¨LLM APIæ—¶å‘ç”Ÿé”™è¯¯: {item[1]}")
                    return None
                if first_chunk:
                    print("âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ:")
                    first_chunk = False
                print(item, end="", flush=True)
                collected_content.append(item)
            print()
            if stream_error:
                print(f"âŒ æµå¼è¯»å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {stream_error[0]}")
                return None
            return "".join(collected_content)

        except Exception as e:
            print(f"âŒ è°ƒç”¨LLM APIæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None


# --- å®¢æˆ·ç«¯ä½¿ç”¨ç¤ºä¾‹ ---
if __name__ == '__main__':
    try:
        llmClient = LLMCompatibleClient(
            model="deepseek-chat",
            apiKey="sk-55950ea43bc44fb58e5379fc9f2c1d2a",
            baseUrl="https://api.deepseek.com",
            timeout=60
        )

        exampleMessages = [
            {"role": "system", "content": "You are a helpful assistant that writes Python code."},
            {"role": "user", "content": "å†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•"}
        ]

        print("--- è°ƒç”¨LLM ---")
        responseText = llmClient.think(exampleMessages)
        if responseText:
            print("\n\n--- å®Œæ•´æ¨¡å‹å“åº” ---")
            print(responseText)

    except ValueError as e:
        print(e)
